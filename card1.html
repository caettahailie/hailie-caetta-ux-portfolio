<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Calibrating Trust in AI – TRUST-CT Case Study</title>
    <link rel="stylesheet" href="case.css">
  </head>
  
<body>
  <div class="page-shell">
    <div class="page-inner">

      <div class="floating-back">
        <a href="index.html#work" class="back-link">← Back to projects</a>
      </div>
      
      

      <main class="case-study">
        
        <header class="case-hero">
          <p class="eyebrow">Capstone Project</p>
          <h1>Calibrating Trust in AI: Designing the TRUST-CT Scale</h1>
          <p class="subtitle">
            A human–AI interaction capstone exploring how interface design and a new scale can support
            healthy skepticism, reduce overreliance, and promote critical thinking with AI-generated text.
          </p>

          <div class="case-meta">
            <div class="case-meta-item">
              <span class="case-meta-label">Role:</span>
              <span>Primary researcher &amp; designer</span>
            </div>
            <div class="case-meta-item">
              <span class="case-meta-label">Course:</span>
              <span>HSE 477 – Capstone</span>
            </div>
            <div class="case-meta-item">
              <span class="case-meta-label">Focus:</span>
              <span>Human–AI trust, automation bias, digital literacy</span>
            </div>
          </div>

          <div class="case-tags">
            <span class="case-tag">Human–AI Interaction</span>
            <span class="case-tag">Trust &amp; Overreliance</span>
            <span class="case-tag">Experimental Design</span>
            <span class="case-tag">Scale Development</span>
          </div>
        </header>

        <div class="case-grid">
          <!-- Left column: narrative -->
          <section>
            <div class="case-section">
              <h2>Project Overview</h2>
              <p>
                As large language models like ChatGPT, Copilot, and Gemini enter everyday workflows,
                people are increasingly relying on AI-generated text to make academic, professional,
                and personal decisions. My capstone project asks:
                <strong>what makes people trust AI responses, and when does that trust actually
                support critical thinking?</strong>
              </p>
              <p>
                I designed <strong>TRUST-CT (Trust for Critical Thinking)</strong>, a user-facing
                scale that measures <em>why</em> someone trusts an AI output and whether that trust is
                likely to lead to healthy verification behaviors instead of blind acceptance.
              </p>
            </div>

            <div class="case-section">
              <h2>The Problem</h2>
              <p>
                Existing trust scales were largely created for traditional automation (like autopilot
                systems), not generative AI that can be <strong>confidently wrong</strong> and
                hallucinate. That mismatch creates two failure modes:
              </p>
              <ul>
                <li><strong>Over-trust / automation bias</strong> – users accept AI outputs without checking.</li>
                <li><strong>Algorithm aversion</strong> – users see one error and reject AI even when it would help.</li>
              </ul>
              <p>
                Many current measures focus on “increasing trust,” but don’t ask whether that trust is
                <strong>appropriately calibrated</strong> or whether the user actually verifies claims.
              </p>
            </div>

            <div class="case-section">
              <h2>Research Goals</h2>
              <p><strong>Main goal:</strong> create a brief, validated scale that captures the reasons
                behind a user’s trust in AI and predicts whether they will double-check the information.</p>
              <p>I focused on three questions:</p>
              <ul>
                <li>What latent factors best explain user trust judgments that trigger verification?</li>
                <li>Does TRUST-CT predict appropriate reliance (accepting correct AI, rejecting incorrect AI)?</li>
                <li>Which interface cues (uncertainty, citations, explanations) most improve trust calibration?</li>
              </ul>
            </div>

            <div class="case-section">
              <h2>TRUST-CT Scale Concept</h2>
              <p>
                TRUST-CT adapts information-literacy frameworks like CRAAP and SIFT to a human–AI trust
                setting. It operationalizes five user-centered factors:
              </p>
              <ul>
                <li><strong>Transparency</strong> – does the AI reveal limits, uncertainty, and gaps?</li>
                <li><strong>Reliability</strong> – does it feel consistent and accurate over time?</li>
                <li><strong>Understandability</strong> – can users follow the reasoning behind a response?</li>
                <li><strong>Source Credibility</strong> – are sources visible, checkable, and trustworthy?</li>
                <li><strong>Teleology (Purpose)</strong> – is the AI’s purpose clear and aligned with user goals?</li>
              </ul>
              <p>
                The scale is intended not only to measure trust but also to
                <strong>nudge users into a brief moment of reflection</strong> when they read an AI-generated claim.
              </p>
            </div>
          </section>

          <!-- Right column: structure / methods / contributions -->
          <aside>
            <div class="case-section">
              <h2>My Role</h2>
              <ul>
                <li>Conducted literature review &amp; gap analysis</li>
                <li>Developed the TRUST-CT conceptual framework and items</li>
                <li>Designed the experimental online study</li>
                <li>Planned psychometric validation (EFA/CFA, reliability)</li>
                <li>Defined analysis pipeline for behavioral outcomes</li>
              </ul>
            </div>

            <div class="case-section">
              <h2>Study Design</h2>
              <p><strong>Participants:</strong> 150–200 adults (18+), fluent in English, with prior AI-tool use.</p>
              <p><strong>Task:</strong> participants read AI-generated statements on neutral topics that vary along:</p>
              <ul>
                <li>Accuracy: correct vs. subtly incorrect (hallucinated)</li>
                <li>Citations: present vs. absent</li>
                <li>Uncertainty cues: present vs. absent</li>
                <li>Explanation type: rationale vs. brief summary</li>
              </ul>
              <p>
                Each person sees a balanced set of statements in a 2×2×2×2 within-subjects design,
                with randomized order to reduce carryover effects.
              </p>
            </div>

            <div class="case-section">
              <h2>Measures &amp; Data</h2>
              <p>For each statement, I collect:</p>
              <ul>
                <li>TRUST-CT item ratings</li>
                <li>A global 1–7 trust rating</li>
                <li>Decision: accept, question, or reject the AI answer</li>
                <li>Whether they click a “Check source” link</li>
                <li>Response time and decision accuracy</li>
              </ul>
              <div class="callout">
                <strong>Why this matters:</strong>
                <ul class="callout-list">
                  <li>Links self-reported trust to real verification behavior</li>
                  <li>Separates “feels trustworthy” from “actually checked it”</li>
                </ul>
              </div>
            </div>

            <div class="case-section">
              <h2>Analysis Plan</h2>
              <p><strong>Scale validation:</strong></p>
              <ul>
                <li>Exploratory Factor Analysis (EFA) on half the sample</li>
                <li>Confirmatory Factor Analysis (CFA) on the other half</li>
                <li>Reliability via Cronbach’s α and McDonald’s ω</li>
              </ul>
              <p><strong>Effect of interface cues:</strong></p>
              <ul>
                <li>Mixed ANOVAs on trust scores, verification, and accuracy</li>
                <li>Logistic regression for binary outcomes (verify vs. not)</li>
                <li>Hierarchical regression to see whether TRUST-CT adds predictive value beyond existing scales</li>
              </ul>
            </div>

            <div class="case-section">
              <h2>Impact &amp; What I Learned</h2>
              <p>
                This work reframes AI design away from “making users trust AI more” toward
                <strong>helping them trust it appropriately</strong>. TRUST-CT is meant to:
              </p>
              <ul>
                <li>Provide a validated, user-facing instrument for AI trust and verification</li>
                <li>Support digital literacy and critical thinking around AI outputs</li>
                <li>Inform interface decisions like uncertainty display, citations, and explanations</li>
              </ul>
              <p>
                As a researcher and designer, this project stretched my skills in experimental design,
                psychometrics, and translating abstract trust concepts into concrete interaction patterns.
              </p>
            </div>
          </aside>
        </div>

        <footer class="case-footer">
          <div>
            <strong>Keywords:</strong> human–AI interaction, trust calibration, automation bias, digital literacy
          </div>
          <div>Capstone project • Arizona State University</div>
        </footer>
      </main>
    </div>
  </div>
</body>
</html>
